[{"entry_id": "http://arxiv.org/abs/2402.07632v1", "updated": "2024-02-12 13:16:30+00:00", "published": "2024-02-12 13:16:30+00:00", "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration", "authors": ["Jingshu Li", "Yitian Yang", "Yi-chieh Lee"], "summary": "As artificial intelligence (AI) advances, human-AI collaboration has become\nincreasingly prevalent across both professional and everyday settings. In such\ncollaboration, AI can express its confidence level about its performance,\nserving as a crucial indicator for humans to evaluate AI's suggestions.\nHowever, AI may exhibit overconfidence or underconfidence--its expressed\nconfidence is higher or lower than its actual performance--which may lead\nhumans to mistakenly evaluate AI advice. Our study investigates the influences\nof AI's overconfidence and underconfidence on human trust, their acceptance of\nAI suggestions, and collaboration outcomes. Our study reveal that disclosing AI\nconfidence levels and performance feedback facilitates better recognition of AI\nconfidence misalignments. However, participants tend to withhold their trust as\nperceiving such misalignments, leading to a rejection of AI suggestions and\nsubsequently poorer performance in collaborative tasks. Conversely, without\nsuch information, participants struggle to identify misalignments, resulting in\neither the neglect of correct AI advice or the following of incorrect AI\nsuggestions, adversely affecting collaboration. This study offers valuable\ninsights for enhancing human-AI collaboration by underscoring the importance of\naligning AI's expressed confidence with its actual performance and the\nnecessity of calibrating human trust towards AI confidence.", "comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "cs.HC"], "links": ["http://arxiv.org/abs/2402.07632v1", "http://arxiv.org/pdf/2402.07632v1"], "pdf_url": "http://arxiv.org/pdf/2402.07632v1"}, {"entry_id": "http://arxiv.org/abs/2401.08915v1", "updated": "2024-01-17 01:59:59+00:00", "published": "2024-01-17 01:59:59+00:00", "title": "How do transportation professionals perceive the impacts of AI applications in transportation? A latent class cluster analysis", "authors": ["Yiheng Qian", "Tejaswi Polimetla", "Thomas W. Sanchez", "Xiang Yan"], "summary": "Recent years have witnessed an increasing number of artificial intelligence\n(AI) applications in transportation. As a new and emerging technology, AI's\npotential to advance transportation goals and the full extent of its impacts on\nthe transportation sector is not yet well understood. As the transportation\ncommunity explores these topics, it is critical to understand how\ntransportation professionals, the driving force behind AI Transportation\napplications, perceive AI's potential efficiency and equity impacts. Toward\nthis goal, we surveyed transportation professionals in the United States and\ncollected a total of 354 responses. Based on the survey responses, we conducted\nboth descriptive analysis and latent class cluster analysis (LCCA). The former\nprovides an overview of prevalent attitudes among transportation professionals,\nwhile the latter allows the identification of distinct segments based on their\nlatent attitudes toward AI. We find widespread optimism regarding AI's\npotential to improve many aspects of transportation (e.g., efficiency, cost\nreduction, and traveler experience); however, responses are mixed regarding\nAI's potential to advance equity. Moreover, many respondents are concerned that\nAI ethics are not well understood in the transportation community and that AI\nuse in transportation could exaggerate existing inequalities. Through LCCA, we\nhave identified four latent segments: AI Neutral, AI Optimist, AI Pessimist,\nand AI Skeptic. The latent class membership is significantly associated with\nrespondents' age, education level, and AI knowledge level. Overall, the study\nresults shed light on the extent to which the transportation community as a\nwhole is ready to leverage AI systems to transform current practices and inform\ntargeted education to improve the understanding of AI among transportation\nprofessionals.", "comment": null, "journal_ref": null, "doi": null, "primary_category": "stat.AP", "categories": ["stat.AP", "cs.CY"], "links": ["http://arxiv.org/abs/2401.08915v1", "http://arxiv.org/pdf/2401.08915v1"], "pdf_url": "http://arxiv.org/pdf/2401.08915v1"}, {"entry_id": "http://arxiv.org/abs/2402.01219v1", "updated": "2024-02-02 08:41:15+00:00", "published": "2024-02-02 08:41:15+00:00", "title": "AI Code Generators for Security: Friend or Foe?", "authors": ["Roberto Natella", "Pietro Liguori", "Cristina Improta", "Bojan Cukic", "Domenico Cotroneo"], "summary": "Recent advances of artificial intelligence (AI) code generators are opening\nnew opportunities in software security research, including misuse by malicious\nactors. We review use cases for AI code generators for security and introduce\nan evaluation benchmark.", "comment": "Dataset available at: https://github.com/dessertlab/violent-python", "journal_ref": "IEEE Security & Privacy, Early Access, February 2024", "doi": "10.1109/MSEC.2024.3355713", "primary_category": "cs.CR", "categories": ["cs.CR", "cs.AI", "cs.SE"], "links": ["http://dx.doi.org/10.1109/MSEC.2024.3355713", "http://arxiv.org/abs/2402.01219v1", "http://arxiv.org/pdf/2402.01219v1"], "pdf_url": "http://arxiv.org/pdf/2402.01219v1"}, {"entry_id": "http://arxiv.org/abs/2402.05048v2", "updated": "2024-02-14 12:02:45+00:00", "published": "2024-02-07 17:41:15+00:00", "title": "How VADER is your AI? Towards a definition of artificial intelligence systems appropriate for regulation", "authors": ["Leonardo C. T. Bezerra", "Alexander E. I. Brownlee", "Luana Ferraz Alvarenga", "Renan Cipriano Moioli", "Thais Vasconcelos Batista"], "summary": "Artificial intelligence (AI) has driven many information and communication\ntechnology (ICT) breakthroughs. Nonetheless, the scope of ICT systems has\nexpanded far beyond AI since the Turing test proposal. Critically, recent AI\nregulation proposals adopt AI definitions affecting ICT techniques, approaches,\nand systems that are not AI. In some cases, even works from mathematics,\nstatistics, and engineering would be affected. Worryingly, AI misdefinitions\nare observed from Western societies to the Global South. In this paper, we\npropose a framework to score how validated as appropriately-defined for\nregulation (VADER) an AI definition is. Our online, publicly-available VADER\nframework scores the coverage of premises that should underlie AI definitions\nfor regulation, which aim to (i) reproduce principles observed in other\nsuccessful technology regulations, and (ii) include all AI techniques and\napproaches while excluding non-AI works. Regarding the latter, our score is\nbased on a dataset of representative AI, non-AI ICT, and non-ICT examples. We\ndemonstrate our contribution by reviewing the AI regulation proposals of key\nplayers, namely the United States, United Kingdom, European Union, and Brazil.\nImportantly, none of the proposals assessed achieve the appropriateness score,\nranging from a revision need to a concrete risk to ICT systems and works from\nother fields.", "comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI", "I.2.0"], "links": ["http://arxiv.org/abs/2402.05048v2", "http://arxiv.org/pdf/2402.05048v2"], "pdf_url": "http://arxiv.org/pdf/2402.05048v2"}, {"entry_id": "http://arxiv.org/abs/2402.11082v1", "updated": "2024-02-16 21:14:11+00:00", "published": "2024-02-16 21:14:11+00:00", "title": "The AI Security Pyramid of Pain", "authors": ["Chris M. Ward", "Josh Harguess", "Julia Tao", "Daniel Christman", "Paul Spicer", "Mike Tan"], "summary": "We introduce the AI Security Pyramid of Pain, a framework that adapts the\ncybersecurity Pyramid of Pain to categorize and prioritize AI-specific threats.\nThis framework provides a structured approach to understanding and addressing\nvarious levels of AI threats. Starting at the base, the pyramid emphasizes Data\nIntegrity, which is essential for the accuracy and reliability of datasets and\nAI models, including their weights and parameters. Ensuring data integrity is\ncrucial, as it underpins the effectiveness of all AI-driven decisions and\noperations. The next level, AI System Performance, focuses on MLOps-driven\nmetrics such as model drift, accuracy, and false positive rates. These metrics\nare crucial for detecting potential security breaches, allowing for early\nintervention and maintenance of AI system integrity. Advancing further, the\npyramid addresses the threat posed by Adversarial Tools, identifying and\nneutralizing tools used by adversaries to target AI systems. This layer is key\nto staying ahead of evolving attack methodologies. At the Adversarial Input\nlayer, the framework addresses the detection and mitigation of inputs designed\nto deceive or exploit AI models. This includes techniques like adversarial\npatterns and prompt injection attacks, which are increasingly used in\nsophisticated attacks on AI systems. Data Provenance is the next critical\nlayer, ensuring the authenticity and lineage of data and models. This layer is\npivotal in preventing the use of compromised or biased data in AI systems. At\nthe apex is the tactics, techniques, and procedures (TTPs) layer, dealing with\nthe most complex and challenging aspects of AI security. This involves a deep\nunderstanding and strategic approach to counter advanced AI-targeted attacks,\nrequiring comprehensive knowledge and planning.", "comment": "SPIE DCS 2024", "journal_ref": null, "doi": null, "primary_category": "cs.CR", "categories": ["cs.CR", "cs.AI"], "links": ["http://arxiv.org/abs/2402.11082v1", "http://arxiv.org/pdf/2402.11082v1"], "pdf_url": "http://arxiv.org/pdf/2402.11082v1"}, {"entry_id": "http://arxiv.org/abs/2401.07058v1", "updated": "2024-01-13 12:19:01+00:00", "published": "2024-01-13 12:19:01+00:00", "title": "Does More Advice Help? The Effects of Second Opinions in AI-Assisted Decision Making", "authors": ["Zhuoran Lu", "Dakuo Wang", "Ming Yin"], "summary": "AI assistance in decision-making has become popular, yet people's\ninappropriate reliance on AI often leads to unsatisfactory human-AI\ncollaboration performance. In this paper, through three pre-registered,\nrandomized human subject experiments, we explore whether and how the provision\nof {second opinions} may affect decision-makers' behavior and performance in\nAI-assisted decision-making. We find that if both the AI model's decision\nrecommendation and a second opinion are always presented together,\ndecision-makers reduce their over-reliance on AI while increase their\nunder-reliance on AI, regardless whether the second opinion is generated by a\npeer or another AI model. However, if decision-makers have the control to\ndecide when to solicit a peer's second opinion, we find that their active\nsolicitations of second opinions have the potential to mitigate over-reliance\non AI without inducing increased under-reliance in some cases. We conclude by\ndiscussing the implications of our findings for promoting effective human-AI\ncollaborations in decision-making.", "comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.HC", "categories": ["cs.HC", "cs.AI"], "links": ["http://arxiv.org/abs/2401.07058v1", "http://arxiv.org/pdf/2401.07058v1"], "pdf_url": "http://arxiv.org/pdf/2401.07058v1"}, {"entry_id": "http://arxiv.org/abs/2401.03223v2", "updated": "2024-01-12 02:14:56+00:00", "published": "2024-01-06 14:23:18+00:00", "title": "An intelligent sociotechnical systems (iSTS) framework: Toward a sociotechnically-based hierarchical human-centered AI approach", "authors": ["Wei Xu", "Zaifeng Gao"], "summary": "Insights: - The human-centered AI (HCAI) approach and the sociotechnical\nsystems (STS) theory share the same goal: ensuring that new technologies such\nas AI best serve humans in a sociotechnical environment. - HCAI practice needs\nto fully embrace sociotechnical systems thinking, while traditional STS needs\nto evolve to address the emerging characteristics of AI technology. - We\npropose a conceptual framework for intelligent sociotechnical systems (iSTS) to\nenhance traditional STS theory in the AI era. - Based on iSTS, we further\npropose a sociotechnical-based hierarchical HCAI approach as a paradigmatic\nextension to existing HCAI practice, further advancing HCAI practice.", "comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.HC", "categories": ["cs.HC"], "links": ["http://arxiv.org/abs/2401.03223v2", "http://arxiv.org/pdf/2401.03223v2"], "pdf_url": "http://arxiv.org/pdf/2401.03223v2"}, {"entry_id": "http://arxiv.org/abs/2401.10273v2", "updated": "2024-01-22 04:47:20+00:00", "published": "2024-01-05 04:01:09+00:00", "title": "Revolutionizing Pharma: Unveiling the AI and LLM Trends in the Pharmaceutical Industry", "authors": ["Yu Han", "Jingwen Tao"], "summary": "This document offers a critical overview of the emerging trends and\nsignificant advancements in artificial intelligence (AI) within the\npharmaceutical industry. Detailing its application across key operational\nareas, including research and development, animal testing, clinical trials,\nhospital clinical stages, production, regulatory affairs, quality control and\nother supporting areas, the paper categorically examines AI's role in each\nsector. Special emphasis is placed on cutting-edge AI technologies like machine\nlearning algorithms and their contributions to various aspects of\npharmaceutical operations. Through this comprehensive analysis, the paper\nhighlights the transformative potential of AI in reshaping the pharmaceutical\nindustry's future.", "comment": null, "journal_ref": null, "doi": null, "primary_category": "cs.CY", "categories": ["cs.CY", "cs.AI"], "links": ["http://arxiv.org/abs/2401.10273v2", "http://arxiv.org/pdf/2401.10273v2"], "pdf_url": "http://arxiv.org/pdf/2401.10273v2"}, {"entry_id": "http://arxiv.org/abs/2401.02799v1", "updated": "2024-01-05 13:28:25+00:00", "published": "2024-01-05 13:28:25+00:00", "title": "Missing Value Chain in Generative AI Governance China as an example", "authors": ["Yulu Pi"], "summary": "We examined the world's first regulation on Generative AI, China's\nProvisional Administrative Measures of Generative Artificial Intelligence\nServices, which came into effect in August 2023. Our assessment reveals that\nthe Measures, while recognizing the technical advances of generative AI and\nseeking to govern its full life cycle, presents unclear distinctions regarding\ndifferent roles in the value chain of Generative AI including upstream\nfoundation model providers and downstream deployers. The lack of distinction\nand clear legal status between different players in the AI value chain can have\nprofound consequences. It can lead to ambiguity in accountability, potentially\nundermining the governance and overall success of AI services.", "comment": "Workshop on Regulatable Machine Learning at the 37th Conference on\n  Neural Information Processing Systems (RegML @ NeurIPS 2023)", "journal_ref": null, "doi": null, "primary_category": "cs.CY", "categories": ["cs.CY"], "links": ["http://arxiv.org/abs/2401.02799v1", "http://arxiv.org/pdf/2401.02799v1"], "pdf_url": "http://arxiv.org/pdf/2401.02799v1"}, {"entry_id": "http://arxiv.org/abs/2308.04586v19", "updated": "2024-02-12 07:50:00+00:00", "published": "2023-08-08 21:14:21+00:00", "title": "Bootstrapping Developmental AIs: From Simple Competences to Intelligent Human-Compatible AIs", "authors": ["Mark Stefik", "Robert Price"], "summary": "Developmental AI is a bootstrapping approach where embodied AIs start with\ninnate competences and learn by interacting with the world. They develop\nabilities in small steps along a bio-inspired trajectory. However,\ndevelopmental AIs have not yet reached the abilities of young children. In\ncontrast, mainstream approaches for creating AIs have led to valuable AI\nsystems and impressive feats. These approaches include deep learning and\ngenerative approaches (e.g., large language models) and manually constructed\nsymbolic approaches. Manually constructed AIs are brittle even in circumscribed\ndomains. Generative AIs are helpful on average, but they can make strange\nmistakes and not notice them. They sometimes lack common sense and social\nalignment. This position paper lays out prospects, gaps, and challenges for\naugmenting AI mainstream approaches with developmental AI. The ambition is to\ncreate data-rich experientially based foundation models and human-compatible,\nresilient, and trustworthy AIs. This research aims to produce AIs that learn to\ncommunicate, establish common ground, read critically, consider the provenance\nof information, test hypotheses, and collaborate. A virtuous multidisciplinary\nresearch cycle has led to developmental AIs with capabilities for multimodal\nperception, object recognition, and manipulation. Computational models for\nhierarchical planning, abstraction discovery, curiosity, and language\nacquisition exist but need to be adapted to an embodied learning approach. They\nneed to bridge competence gaps involving nonverbal communication, speech,\nreading, and writing. Aspirationally, developmental AIs would learn, share what\nthey learn, and collaborate to achieve high standards. The approach would make\nthe creation of AIs more democratic, enabling more people to train, test, build\non, and replicate AIs.", "comment": "121 pages, 28 figures, 4 tables", "journal_ref": null, "doi": null, "primary_category": "cs.AI", "categories": ["cs.AI"], "links": ["http://arxiv.org/abs/2308.04586v19", "http://arxiv.org/pdf/2308.04586v19"], "pdf_url": "http://arxiv.org/pdf/2308.04586v19"}]